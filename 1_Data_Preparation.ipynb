{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bike-Share Demand Forecasting 1: Data Preparation\n",
    "\n",
    "이번 timeseries forecasting 예제는 날씨에 관련성이 높은 [Capital Bikeshare scheme in 2011-12](https://www.capitalbikeshare.com/system-data)를 이용하여 bikeshare의 대여 수요를 예측하는 것입니다.\n",
    "\n",
    "이 노트북은 UC Irvine website에서 [original weather-annotated dataset](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset)를 다운로드 받으며, S3 업로드 전에 기본적인 데이터 전처리 과정을 수행합니다.\n",
    "\n",
    "이후 노트북에서는 S3로 업로드된 데이터셋을 이용하여 모델을 fit한 다음 정확도를 비교하게 됩니다.\n",
    "\n",
    "### 1. Dataset 소개\n",
    "\n",
    " - 멤버쉽, 렌탈 및 자전거 반납 등의 프로세스가 자동으로 이루어지는 자전거 렌탈과 관련된 자전거 공유 시스템의 데이터입니다.\n",
    " - 데이터셋은 hour.csv와 day.csv를 가지고 있으며, 아래 구성 필드로 이루어져 있습니다. [단, day.csv는 hr(시간) 필드 제외]\n",
    "    - instant: 기록된 index\n",
    "    - dteday : 대여 날짜\n",
    "    - season : 계절 데이터 (1:winter, 2:spring, 3:summer, 4:fall)\n",
    "    - yr : 연도 (0: 2011, 1:2012)\n",
    "    - mnth : 월 데이터 ( 1 to 12)\n",
    "    - hr : 시각 (0 to 23)\n",
    "    - holiday : 휴일 여부\n",
    "    - weekday : 평일\n",
    "    - workingday : 주중이면 1, 주말 또는 휴일이면 0\n",
    "    - weathersit :\n",
    "        - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "        - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "        - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "        - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "    - temp : Normalized 온도 (Celsius), 계산식 : (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39 (only in hourly scale)\n",
    "    - atemp: Normalized 체감 온도 (Celsius), 계산식 : (t-t_min)/(t_max-t_min), t_min=-16, t_max=+50 (only in hourly scale)\n",
    "    - hum: Normalized 습도 (The values are divided to 100 (max))\n",
    "    - windspeed: Normalized 풍속 (The values are divided to 67 (max))\n",
    "    - casual: 미동록 사용자의 대여 횟수 \n",
    "    - registered: 등록 사용자의 대여 횟수\n",
    "    - cnt: 전체 자전가 대여 횟수 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies and configuration<a class=\"anchor\" id=\"setup\"/>\n",
    "\n",
    "라이브러리를 로딩한 다음, 설정값을 정의하고, AWS SDKs에 연결합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = 'XXX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "\n",
    "# Local Dependencies:\n",
    "%aimport util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img src='./BlogImages/add_img/dataset_com.png' width=700 height=600></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### CloudFormation으로 만들어진 S3 bucket 이름을 아래 넣습니다.\n",
    "<p><img src='./BlogImages/add_img/s3bucket.png' width=700 height=500></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'forecast-demolab-' + initial# TODO: Choose a bucket you've created, and SageMaker has full access to\n",
    "%store bucket\n",
    "\n",
    "# Data files to be stored in S3:\n",
    "data_prefix = \"data/\"\n",
    "%store data_prefix\n",
    "target_train_filename = \"target_train.csv\"\n",
    "%store target_train_filename\n",
    "target_test_filename = \"target_test.csv\"\n",
    "%store target_test_filename\n",
    "related_filename = \"related.csv\"\n",
    "%store related_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "boto3 이름의 AWS SDKs python package를 이용하여 s3에 업로드 하기 위한 session을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session()\n",
    "region = session.region_name\n",
    "s3 = session.client(service_name=\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Fetch the source data<a class=\"anchor\" id=\"fetch\"/>\n",
    "\n",
    "데이터 셋은 비교적 작은 편이기에 우르는 큰 디스크 할당 없이 노트북 인스턴스에서 데이터를 처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O data.zip http://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip\n",
    "!rm -rf ./data/raw\n",
    "!mkdir -p ./data/raw\n",
    "!unzip data.zip -d ./data/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [UCI dataset documentation](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset)를 column headers 기준으로 pandas python package를 이용하여 메모리에 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(\n",
    "    \"./data/raw/hour.csv\",\n",
    "    index_col=\"instant\",\n",
    "    dtype={\n",
    "        \"dteday\": str,\n",
    "        \"season\": int,\n",
    "        \"yr\": int,\n",
    "        \"mnth\": int,\n",
    "        \"hr\": int,\n",
    "        \"holiday\": bool,\n",
    "        \"weekday\": int,\n",
    "        \"workingday\": bool,\n",
    "        \"weathersit\": int,\n",
    "        \"temp\": float,\n",
    "        \"atemp\": float,\n",
    "        \"hum\": float,\n",
    "        \"windspeed\": float,\n",
    "        \"casual\": int,\n",
    "        \"registered\": int,\n",
    "        \"cnt\": int\n",
    "    }\n",
    ").sort_values([\"dteday\", \"hr\"])\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가적으로 각 컬럼들의 범위와 variation을 빠르게 확인할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Source Data Analysis<a class=\"anchor\" id=\"fetch\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 전체 데이터셋을 이용하여 전체 주기 동안의 수요(자전거 대여횟수)를 그래프화하여 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_df = raw_df.copy()\n",
    "\n",
    "# Combine day and hour into datetime column, and drop superfluous date features:\n",
    "hourly_df[\"dteday\"] = pd.to_datetime(raw_df[\"dteday\"].map(str) + \" \" + raw_df[\"hr\"].map(str) + \":00\")\n",
    "hourly_df = hourly_df.rename(columns={ \"dteday\": \"timestamp\" }).drop(columns=[\"yr\", \"mnth\", \"hr\"])\n",
    "\n",
    "daterange = pd.date_range(\n",
    "    start=list(hourly_df[\"timestamp\"])[0],\n",
    "    end=list(hourly_df[\"timestamp\"])[-1],\n",
    "    freq='H'\n",
    ")\n",
    "tmp = pd.DataFrame({ \"timestamp\": daterange })\n",
    "tmp[\"dteday\"] = tmp[\"timestamp\"].dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "fill_df = tmp.merge(\n",
    "    hourly_df,\n",
    "    how=\"left\",\n",
    "    on=\"timestamp\"\n",
    ").drop(columns=[\"dteday\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 8))\n",
    "ax = plt.gca()\n",
    "fill_df.plot(x=\"timestamp\", y=\"registered\", ax=ax, label=\"Registered Customers\")\n",
    "fill_df.plot(x=\"timestamp\", y=\"casual\", ax=ax, label=\"Casual Customers\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of Trips\")\n",
    "ax.set_title(\"Comparison of Registered and Casual Demand - Whole Data Set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그래프에서 우리는 아래 내용을 알 수 있습니다.\n",
    "\n",
    "1. 등록된 고객들이 대부분의 demand를 차지합니다.\n",
    "2. 비록 여름/겨울 계절성을 데이터 내에서 볼 수 있지만, 서비스의 전체 증가 및 인기도는 2년 사이 전반적으로 성장하는 것을 볼 수 있습니다.\n",
    "3. 전반적인 데이터에서 볼 수 있는 것 보다 단기간 내 강한 주기성 (spikiness)가 있습니다.\n",
    "\n",
    "** 2번은 forecast의 정확도 관점에서 어려운 상황이긴 합니다. **\n",
    "- 단지 2년의 데이터만을 가지고 있으며 연간 seasonaility가 중요하지만, 일반적으로는 중요 패턴에 대해서는 5배 더 긴 주기를 가는 과거 이력이 필요합니다.\n",
    "- 서비스의 전반적인 수요는 non-stationary 한 것으로 판단되며, 이는 많은 forecasting 방법에서 정확도를 떨어뜨릴 수 있습니다.[here](https://cs.nyu.edu/~mohri/talks/NIPSTutorial2016.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래 그래프에서는 수요에 대해 요일과 시간의 영향이 명확히 볼 수 있으며, 적어도 여름인 8월에 casual 라이더의 경우 주 보다 주말의 여행 비율이 높다는 것을 알 수 있습니다.\n",
    "- 짧은 timescales에서는 데이터는 더욱 stationary 한 것을 볼 수 있고, 우리는 더욱 robust한 forecast 결과를 얻을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_df = fill_df[(fill_df[\"timestamp\"] >= \"2012-08-01\") & (fill_df[\"timestamp\"] < \"2012-09-01\")]\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "ax = plt.gca()\n",
    "period_df.plot(x=\"timestamp\", y=\"registered\", ax=ax, label=\"Registered Customers\")\n",
    "period_df.plot(x=\"timestamp\", y=\"casual\", ax=ax, label=\"Casual Customers\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of Trips\")\n",
    "ax.set_title(\"Comparison of Registered and Casual Demand - August 2012\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Trends and Outliers Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=11,ncols=2)\n",
    "fig.set_size_inches(20, 60)\n",
    "sn.boxplot(data=raw_df,y=\"casual\",orient=\"v\",ax=axes[0][0])\n",
    "sn.boxplot(data=raw_df,y=\"registered\",orient=\"v\",ax=axes[0][1])\n",
    "\n",
    "axes[0][0].set(ylabel='Casual',title=\"Box Plot On Casual\")\n",
    "axes[0][1].set(ylabel='Registered',title=\"Box Plot On Registered\")\n",
    "\n",
    "x_axises = ['season', 'mnth', 'hr', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed']\n",
    "\n",
    "for i, x_axis in enumerate(x_axises):\n",
    "    for j, y_axis in enumerate(['casual', 'registered']):\n",
    "        sn.boxplot(data=raw_df,y=y_axis,x=x_axis,orient=\"v\",ax=axes[i+1][j])\n",
    "        \n",
    "x_labels = ['Season', 'Months', 'Hours', 'Weekday', 'Working Day', 'Weathersit', 'Temp', 'aTemp', 'Hum', 'Windspeed']\n",
    "\n",
    "for i, x_label in enumerate(x_labels):\n",
    "    for j, y_label in enumerate(['Casual', 'Registered']):\n",
    "        title = \"Box Plot On \" + y_label + \" Across \" + x_label\n",
    "        axes[i+1][j].set(xlabel=x_label, ylabel=y_label, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(fill_df,figsize=(20,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 feature 대비 종속 변수가 어떻게 영향을 받는지 확인하기 위해 아래 상관 행렬을 만들어 봅니다.\n",
    "\n",
    "   - 종속변수 ```cnt``` 대비하여 ```temp/atemp```는 양의 상관관계를 가지며, ```hum```은 음의 상관관계를 갖는 것을 확인할 수 있습니다.\n",
    "   - ```windspeed```는 상관성이 낮은 것으로 판단됩니다.\n",
    "   - ```atemp```는 ```temp```와 상호 높은 상관 관계를 가지므로 둘 중 하나의 데이터는 삭제하는 것이 좋습니다.  \n",
    "     *multicollinearity 문제 : 독립변수 간의 상관관계가 발생할 경우 삭제하는 것이 좋음*\n",
    "   - ```casual```과 ```registered```는 종속변수 ```cnt```를 분리한 값이므로 이후 모델링 시에는 삭제합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMatt = fill_df[[\"temp\",\"atemp\",\"casual\",\"registered\",\"hum\",\"windspeed\",\"cnt\"]].corr()\n",
    "mask = np.array(corrMatt)\n",
    "mask[np.tril_indices_from(mask)] = False\n",
    "fig,ax= plt.subplots()\n",
    "fig.set_size_inches(20,10)\n",
    "sn.heatmap(corrMatt, mask=mask,vmax=.8, square=True,annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load data & interpolate gaps<a class=\"anchor\" id=\"convert\"/>\n",
    "\n",
    "The raw dataset has:\n",
    "\n",
    "* 날짜 / 시간 열 중복 제거 필요\n",
    "* 대여 횟수가 없는 시간은 raw dataset에서 나타나지 않음\n",
    "\n",
    "...which we'll deal with here, starting with the timestamp tidy-up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_df = raw_df.copy()\n",
    "\n",
    "# Combine day and hour into datetime column, and drop superfluous date features:\n",
    "hourly_df[\"dteday\"] = pd.to_datetime(raw_df[\"dteday\"].map(str) + \" \" + raw_df[\"hr\"].map(str) + \":00\")\n",
    "hourly_df = hourly_df.rename(columns={ \"dteday\": \"timestamp\" }).drop(columns=[\"yr\", \"mnth\", \"hr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아무도 자전거를 사용하지 않은 시간 동안 행은 누락되었지만 적용 범위 내 매일 최소 하나의 행은 존재합니다.\n",
    "결측치는 아래와 같이 처리하였습니다.\n",
    "\n",
    " 1. 라이더 수 (casual, registered, cnt) 는 0으로 설정 (cnt=0 은 행 대체가 적용된 경우 복구 시 사용)\n",
    " 2. 일별 값 (season, holiday, weekday, workingday)은 해당 날짜의 원본 레코드의 평균값을 사용\n",
    " 3. 날씨 데이터 (weathersit, temp, atemp, hum, windspeed): 시간 상 현재 레코드들과 가장 근접한 값들로 보간\n",
    "\n",
    "날씨 데이터는 유일한 대체이며 시간별 보간으로 시간별 날씨 데이터의 결측 시간을 채우게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the full range of hours covered by the data-set, to the number of records:\n",
    "daterange = pd.date_range(\n",
    "    start=list(hourly_df[\"timestamp\"])[0],\n",
    "    end=list(hourly_df[\"timestamp\"])[-1],\n",
    "    freq='H'\n",
    ")\n",
    "\n",
    "n_raw_records = len(hourly_df)\n",
    "n_range_hours = len(daterange)\n",
    "print(f\"{n_raw_records} raw records vs {n_range_hours} hours in date range\")\n",
    "if (n_raw_records == n_range_hours):\n",
    "    print(\"Data fully specified\")\n",
    "elif (n_raw_records < n_range_hours):\n",
    "    # (We expect to see this)\n",
    "    print(\"MISMATCH: Missing data will be interpolated\")\n",
    "elif (n_raw_records > n_range_hours):\n",
    "    raise NotImplementedError(\"This script can't deal with duplicates yet!\")\n",
    "\n",
    "# Construct a fully range-covering table\n",
    "# (including day-granularity fields taken from aggregating the source table)\n",
    "tmp = pd.DataFrame({ \"timestamp\": daterange })\n",
    "tmp[\"dteday\"] = tmp[\"timestamp\"].dt.strftime(\"%Y-%m-%d\")\n",
    "fill_df = tmp.merge(\n",
    "    raw_df.groupby(\"dteday\").agg(\"mean\")[[\"season\", \"holiday\", \"weekday\", \"workingday\"]],\n",
    "    how=\"left\",\n",
    "    on=\"dteday\"\n",
    ").drop(columns=[\"dteday\"])\n",
    "\n",
    "# Join the whole-range table to our target, and fill in the day-granularity fields\n",
    "assert (\n",
    "    hourly_df.isna().sum().sum() == 0\n",
    "), \"These imputations assume no missing values in source data set records!\"\n",
    "\n",
    "imputed_df = fill_df.merge(hourly_df, how=\"left\", on=\"timestamp\", suffixes=(\"_day\", \"\"))\n",
    "imputed_df[\"season\"].fillna(imputed_df[\"season_day\"], inplace=True)\n",
    "imputed_df[\"holiday\"].fillna(imputed_df[\"holiday_day\"], inplace=True)\n",
    "imputed_df[\"weekday\"].fillna(imputed_df[\"weekday_day\"], inplace=True)\n",
    "imputed_df[\"workingday\"].fillna(imputed_df[\"workingday_day\"], inplace=True)\n",
    "imputed_df.drop(columns=[\"season_day\", \"holiday_day\", \"weekday_day\", \"workingday_day\"], inplace=True)\n",
    "\n",
    "# Fill all missing demand values with zero (which is why the records were missing in the first place)\n",
    "imputed_df[\"casual\"].fillna(0, inplace=True)\n",
    "imputed_df[\"registered\"].fillna(0, inplace=True)\n",
    "imputed_df[\"cnt\"].fillna(0, inplace=True)\n",
    "\n",
    "# Interpolate over time for missing weather data fields:\n",
    "imputed_df = imputed_df.set_index(\"timestamp\")\n",
    "imputed_df[\"weathersit\"] = imputed_df[\"weathersit\"].interpolate(method=\"time\").round()\n",
    "imputed_df[\"temp\"].interpolate(method=\"time\", inplace=True)\n",
    "imputed_df[\"atemp\"].interpolate(method=\"time\", inplace=True)\n",
    "imputed_df[\"hum\"].interpolate(method=\"time\", inplace=True)\n",
    "imputed_df[\"windspeed\"].interpolate(method=\"time\", inplace=True)\n",
    "imputed_df = imputed_df.reset_index()\n",
    "\n",
    "assert not imputed_df.isna().any().any(), \"Imputed DF should not have any remaining nulls!\"\n",
    "assert len(imputed_df) == n_range_hours, \"Imputed DF should fully cover time range!\"\n",
    "\n",
    "print(\"Imputation complete\")\n",
    "imputed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to extend feature engineering here, just leaving timestamp and *_demand columns alone:\n",
    "target_suffix = \"_demand\"\n",
    "full_df = imputed_df.rename(columns={\n",
    "    \"casual\": f\"casual{target_suffix}\",\n",
    "    \"registered\": f\"registered{target_suffix}\",\n",
    "    \"cnt\": f\"total{target_suffix}\"\n",
    "})\n",
    "\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train/test split for eodel evaluation<a class=\"anchor\" id=\"split\"/>\n",
    "\n",
    "feature engineering 작업을 수행한 다음, 고민해야 할 가장 중요한 의문점은 모델의 정확도를 평가하는 방법입니다.\n",
    "\n",
    "### How to evaluate forecast models\n",
    "\n",
    "forecasting 알고리즘을 평가하는 것과 일반적인 ML 방법과의 중요한 차이는 **casuality (인과관계)** 입니다. 과거 데이터 기반으로 우리의 모델을 fitting 해야 합니다. 그리고, 알수 없는 \"미래\" 데이터를 평가합니다. 모델의 fitting 프로세스에서는 미래의 데이터가 보이지 않도록 합니다.\n",
    "\n",
    "**[Backtesting](https://en.wikipedia.org/wiki/Backtesting)** (or \"hindcasting\")는 이런 인관관계를 평가하는 프로세스입니다. 과거 데이터에서 시점 기준으로 하나 이상의 포인트를 선택하는 방법으로 cut-off 시나리오에서 사용합니다. 포인트 이전 데이터는 학습 데이터로 사용하고, 이후 데이터는 평가용으로 사용합니다.  \n",
    "\n",
    "<img src=\"BlogImages/backtest.png\"/>\n",
    "\n",
    "다중의 timeseries를 갖는 상황에서, Target Timeseries와 Related Timeseries의 차이점을 아래 그림으로 알 수 있습니다.\n",
    "\n",
    "* **target timeseries** (모델이 예측하고자 하는 값들), and\n",
    "* **[related timeseries](https://docs.aws.amazon.com/forecast/latest/dg/related-time-series-datasets.html)** (예측 기간에 대해 우리가 미리 알 수 있는 값들)\n",
    "\n",
    "따라서, target timeseries에 대해서만 train/test split을 수행합니다.\n",
    "\n",
    "\n",
    "<img src=\"BlogImages/rts_viz.png\">\n",
    "\n",
    "\n",
    "### Choosing the Setup for our Bike Forecasting example\n",
    "\n",
    "소스 데이터는 **hourly granularity**를 가지고 있으며, 이는 예측 가능한 단위의 가장 하한 값입니다.\n",
    "이 granularity를 사용하거나 또는 aggregate up할지를 선택하는 것은 적절한 **forecast horizon**가 무엇인지를 가이드할 것입니다.\n",
    "\n",
    "* 모델 학습은 주어진 forecast horizon에 대해 전체 정확도를 최적화할 것입니다. 따라서, 대부분의 유용한 horizon은 **비즈니스 문제**에 따라 선정해야 합니다. 예를 들어, 최선을 다해 1년을 예측하는 모델을 학습했다고 하지만, 실제 1개월 계획을 세우기 위해 사용하는 경우에는 성능이 떨어질 수 있습니다.\n",
    "* DeepAR과 같은 RNN (Recurrent Neural Models)과도 비교할 것이기 때문에, granularity가 적어도 forecast window의 특정 비율일 때 이 아키텍처들의 일부에서 나쁘게 작동한다는 점도 고려할 필요가 있습니다. 어떤 패턴의 주기를 학습 할 수 있는지에 대한 제약 사항은 샘플 주기에 비례합니다. 특히, Amazon Forecast는 최대 500개의 샘플 forecast horizon까지 가능합니다. [limit](https://docs.aws.amazon.com/forecast/latest/dg/limits.html)\n",
    "* 알고리즘은 context window 내 데이터가 거의 **stationary**하고 가장 긴 fluctuations의 여러 사이클을 잡아낼 때 최상으로 수행될 것입니다. 분기 또는 연간 forecasts는 사용 가능한 데이터셋에 대해 적합하지 않을 수 있습니다.\n",
    "\n",
    "또한,\n",
    "\n",
    "* (적어도 일부 국가에서) 괜찮은 날씨 예측이 가능한 기간과 수요에 적합하도록 bikes의 이동 및 배치를 위해 적합한 사전 planning 단계를 수행할 수 있는 기간 사이의 대략적인 평균을 고려하고,\n",
    "* 시간별 샘플이 유지될 정도로 짧은 기간이며, 집계가 적절한 경우를 고려하여, \n",
    "\n",
    "이번에는 **14일 (2주) horizon**에 대한 predictors를 학습할 것입니다.\n",
    "\n",
    "연말 기간이 business 적으로 중요한 시간임으로 가정하여, 2012-12-01 까지를 최종 테스트 cutoff로 offset합니다. final evaluation window는 단지 2년의 데이터를 제공하는 상황이기 때문에 모델에 대해 연간 seasonality의 노출을 최대화합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='BlogImages/add_img/dataset.png' width=900 height=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_test_start = \"2012-12-01\"\n",
    "\n",
    "full_headers = full_df.columns.to_list()\n",
    "timestamp_header = \"timestamp\"\n",
    "target_headers = list(filter(lambda s: s.endswith(target_suffix), full_headers))\n",
    "target_nontotal_headers = list(filter(lambda s: s != \"total_demand\", target_headers))\n",
    "related_headers = list(filter(lambda s: (s not in target_headers) and (s != timestamp_header), full_headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpivoted dataframe of target variables, still sorted by timestamp:\n",
    "target_full_df = full_df[[timestamp_header] + target_nontotal_headers].melt(\n",
    "    id_vars=[timestamp_header],\n",
    "    value_vars=target_nontotal_headers,\n",
    "    var_name=\"customer_type\",\n",
    "    value_name=\"demand\"\n",
    ").sort_values(by=[timestamp_header, \"customer_type\"]).reset_index(drop=True)\n",
    "\n",
    "# Strip \"_demand\" from the target IDs:\n",
    "target_full_df[\"customer_type\"] = target_full_df[\"customer_type\"].apply(lambda s: s[0:-len(target_suffix)])\n",
    "\n",
    "target_full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train_df = target_full_df[target_full_df[timestamp_header] < cutoff_test_start]\n",
    "target_test_df = target_full_df[target_full_df[timestamp_header] >= cutoff_test_start]\n",
    "\n",
    "related_df = full_df[[timestamp_header] + related_headers]\n",
    "\n",
    "print(f\"{len(target_train_df)} target training points\")\n",
    "print(f\"{len(target_test_df)} target test points\")\n",
    "print(f\"{len(related_df)} related timeseries points\")\n",
    "\n",
    "related_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing dataframes to file...\")\n",
    "target_train_df.to_csv(\n",
    "    f\"./data/{target_train_filename}\",\n",
    "    index=False\n",
    ")\n",
    "target_test_df.to_csv(\n",
    "    f\"./data/{target_test_filename}\",\n",
    "    index=False\n",
    ")\n",
    "related_df.to_csv(\n",
    "    f\"./data/{related_filename}\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"Uploading dataframes to S3...\")\n",
    "s3.upload_file(\n",
    "    Filename=f\"./data/{target_train_filename}\",\n",
    "    Bucket=bucket,\n",
    "    Key=f\"{data_prefix}{target_train_filename}\"\n",
    ")\n",
    "s3.upload_file(\n",
    "    Filename=f\"./data/{target_test_filename}\",\n",
    "    Bucket=bucket,\n",
    "    Key=f\"{data_prefix}{target_test_filename}\"\n",
    ")\n",
    "s3.upload_file(\n",
    "    Filename=f\"./data/{related_filename}\",\n",
    "    Bucket=bucket,\n",
    "    Key=f\"{data_prefix}{related_filename}\"\n",
    ")\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 데이터셋은 S3로 업로드를 했으며, 모델의 학습을 수행한 다음 성능 비교가 가능합니다.! 실제 Forecast를 수행하기 위해 다음 notebook으로 이동해주시기 바랍니다. \n",
    "\n",
    "\n",
    "[2a Amazon Forecast Model](2a_Amazon_Forecast_Model.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
