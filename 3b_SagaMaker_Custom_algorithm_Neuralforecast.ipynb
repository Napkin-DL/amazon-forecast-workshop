{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural ğŸ§  Forecast\n",
    "--------------\n",
    "ì´ ì‹¤ìŠµì€ https://nixtla.github.io/neuralforecast/ ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ SageMakerì—ì„œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ê°€ì´ë“œí•˜ê³ ì ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ëª¨ë“  ë¼ì´ì„ ìŠ¤ëŠ” [ì—¬ê¸°](https://github.com/Nixtla/neuralforecast/blob/main/LICENSE) êµ¬í˜„ëœ ì›ë³¸ ì†ŒìŠ¤ì½”ë“œì˜ ë¼ì´ì„ ìŠ¤ ì •ì±…ì„ ë”°ë¥´ê³  ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë° ì—…ë°ì´íŠ¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. í™˜ê²½ ì„¤ì •\n",
    "\n",
    "Sagemaker í•™ìŠµì— í•„ìš”í•œ ê¸°ë³¸ì ì¸ packageë¥¼ import í•©ë‹ˆë‹¤. <br>\n",
    "[boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)ëŠ” AWS ë¦¬ì†ŒìŠ¤ì™€ ë™ì‘í•˜ëŠ” python í´ë˜ìŠ¤ë¥¼ ì œê³µí•˜ë©°, HTTP API í˜¸ì¶œì„ ìˆ¨ê¸°ëŠ” ì¶”ìƒí™” ëª¨ë¸ì…ë‹ˆë‹¤. boto3ë¥¼ í†µí•´ pythonì—ì„œ Amazon EC2 ì¸ìŠ¤í„´ìŠ¤, S3 ë²„ì¼“ê³¼ ê°™ì€ AWS ë¦¬ì†ŒìŠ¤ì™€ ë™ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.<br>\n",
    "[sagemaker python sdk](https://sagemaker.readthedocs.io/en/stable/)ëŠ” Amazon SageMakerì—ì„œ ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì„ êµìœ¡ ë° ë°°í¬í•˜ê¸° ìœ„í•œ ì˜¤í”ˆ ì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sagemaker\n",
    "# import splitfolders\n",
    "\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# from smexperiments.experiment import Experiment\n",
    "# from smexperiments.trial import Trial\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from time import strftime\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Dataset ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic data configuration is initialised and stored in the Data Preparation notebook\n",
    "# ...We just retrieve it here:\n",
    "%store -r\n",
    "assert bucket, \"Variable `bucket` missing from IPython store\"\n",
    "assert data_prefix, \"Variable `data_prefix` missing from IPython store\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ì‹¤í—˜ ì„¤ì •\n",
    "\n",
    "í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ì†ŒìŠ¤ì½”ë“œì™€ output ì •ë³´ë¥¼ ì €ì¥í•  ìœ„ì¹˜ë¥¼ ì„ ì •í•©ë‹ˆë‹¤. ì´ ê°’ì€ í•„ìˆ˜ë¡œ ì„¤ì •í•˜ì§€ ì•Šì•„ë„ ë˜ì§€ë§Œ, ì½”ë“œì™€ ê²°ê³¼ë¬¼ì„ S3ì— ì €ì¥í•  ë•Œ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•˜ëŠ”ë° í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "code_location = f's3://{bucket}/poc_neuralforecast/sm_codes'\n",
    "output_path = f's3://{bucket}/poc_neuralforecast/output' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‹¤í—˜ì—ì„œ í‘œì¤€ ì¶œë ¥ìœ¼ë¡œ ë³´ì—¬ì§€ëŠ” metrics ê°’ì„ ì •ê·œ í‘œí˜„ì‹ì„ ì´ìš©í•˜ì—¬ SageMakerì—ì„œ ê°’ì„ captureí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê°’ì€ í•„ìˆ˜ë¡œ ì„¤ì •í•˜ì§€ ì•Šì•„ë„ ë˜ì§€ë§Œ, SageMaker Experimentsì— Metrics ì •ë³´ë¥¼ ë‚¨ê¸¸ ìˆ˜ ìˆì–´ì„œ ì‹¤í—˜ ê´€ë¦¬ì— ìœ ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    {'Name': 'Epoch', 'Regex': 'Epoch ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?):'},\n",
    "    {'Name': 'Train Loss Step', 'Regex': 'train_loss_step=([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'Train Loss Epoch', 'Regex': 'train_loss_epoch=([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'valid_loss', 'Regex': 'valid_loss=([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ì–‘í•œ ì‹¤í—˜ ì¡°ê±´ì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´ hyperparametersë¡œ argument ê°’ë“¤ì„ ë…¸íŠ¸ë¶ì—ì„œ ì„¤ì •í•  ìˆ˜ ìˆìœ¼ë©°, ì´ ê°’ì€ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ argumentì¸ ë³€ìˆ˜ë¡œ ë°›ì•„ì„œ í™œìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'horizon' : 336,\n",
    "    'num_samples' : 5,\n",
    "    'freq' : 'H',\n",
    "    'cv_use' : True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë¶„ì‚°í•™ìŠµê³¼ spot í•™ìŠµì„ ì‚¬ìš©í• ì§€ë¥¼ ì„ ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. <br>\n",
    "ë¶„ì‚°í•™ìŠµì˜ ê²½ìš° [SageMaker data parallel library](https://docs.aws.amazon.com/sagemaker/latest/dg/data-parallel.html)ë¥¼ ì‚¬ìš©í•˜ê³ ì í•  ê²½ìš° distributionì„ ì•„ë˜ì™€ ê°™ì´ ì„¤ì •í•œ í›„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë¶„ì‚° í•™ìŠµ Libraryë¡œ êµ¬í˜„ì´ í•„ìš”í•©ë‹ˆë‹¤. <br>\n",
    "[spot í•™ìŠµ](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html)ì„ ì‚¬ìš©í•˜ê³ ì í•  ê²½ìš° í•™ìŠµ íŒŒë¼ë¯¸í„°ì— spot íŒŒë¼ë¯¸í„°ë¥¼ Trueë¡œ ë³€ê²½í•œ ë‹¤ìŒ, ìì›ì´ ì—†ì„ ë•Œ ëŒ€ê¸°í•˜ëŠ” ì‹œê°„ì¸ max_wait (ì´ˆ)ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "experiment_name = 'neuralforecast-poc-exp1'\n",
    "distribution = None\n",
    "do_spot_training = True\n",
    "max_wait = None\n",
    "max_run = 4*60*60\n",
    "\n",
    "if do_spot_training:\n",
    "    max_wait=max_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instance_type=\"ml.g5.2xlarge\"\n",
    "# instance_type='local_gpu'\n",
    "instance_count=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. checkpoints ì„¤ì •, ë°ì´í„° ìœ„ì¹˜ ì„¤ì •, Local Mode ì„¤ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-2. checkpointsì™€ ë°ì´í„° ìœ„ì¹˜ ì„¤ì •, Local Mode ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "source_dir = f'{Path.cwd()}/neuralforecast'\n",
    "    \n",
    "if instance_type =='local_gpu' or instance_type =='local':\n",
    "    from sagemaker.local import LocalSession\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    inputs = f'file://{Path.cwd()}/data/amzforecast'\n",
    "\n",
    "    do_spot_training = False\n",
    "    checkpoint_s3_uri=None\n",
    "    max_wait = None\n",
    "else:\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    inputs = f's3://{bucket}/data/amzforecast'\n",
    "    checkpoint_s3_uri = f's3://{bucket}/poc_neuralforecast/checkpoints'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. í•™ìŠµì„ ìœ„í•œ Estimator ì„ ì–¸\n",
    "\n",
    "AWS ì„œë¹„ìŠ¤ í™œìš© ì‹œ role (ì—­í• ) ì„¤ì •ì€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ ë…¸íŠ¸ë¶ì—ì„œ ì‚¬ìš©í•˜ëŠ” roleì€ ë…¸íŠ¸ë¶ê³¼ training jobì„ ì‹¤í–‰í•  ë•Œ ì‚¬ìš©í•˜ëŠ” roleì´ë©°, roleì„ ì´ìš©í•˜ì—¬ ë‹¤ì–‘í•œ AWS ì„œë¹„ìŠ¤ì— ëŒ€í•œ ì ‘ê·¼ ê¶Œí•œì„ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# all input configurations, parameters, and metrics specified in estimator \n",
    "# definition are automatically tracked\n",
    "estimator = PyTorch(\n",
    "    entry_point='main.py',\n",
    "    source_dir=source_dir,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    framework_version='2.0',\n",
    "    py_version='py310',\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "#     volume_size=256,\n",
    "    code_location = code_location,\n",
    "    output_path=output_path,\n",
    "    hyperparameters=hyperparameters,\n",
    "    # distribution=distribution,\n",
    "    metric_definitions=metric_definitions,\n",
    "    max_run=max_run,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    use_spot_instances=do_spot_training,\n",
    "    max_wait=max_wait,\n",
    "    disable_profiler=True,\n",
    "    debugger_hook_config=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. í•™ìŠµ ìˆ˜í–‰ - ì‹œì‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_date = strftime(\"%m%d-%H%M%s\")\n",
    "job_name = f\"{experiment_name}-{create_date}\"\n",
    "\n",
    "# Now associate the estimator with the Experiment and Trial\n",
    "estimator.fit(\n",
    "    inputs={'training': inputs}, \n",
    "    job_name=job_name,\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job_name=estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì´ìš©í•˜ì—¬ ì‹œì‘ëœ í•™ìŠµì— ëŒ€í•œ ë¡œê·¸ë¥¼ ë…¸íŠ¸ë¶ì—ì„œ í™•ì¸í•©ë‹ˆë‹¤. ì´ ë¡œê·¸ëŠ” CloudWatchì—ì„œë„ í™•ì¸ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. <br> \n",
    "ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•´ë„ í•™ìŠµì´ ì‹œì‘ë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë©°, ì‹¤í–‰ëœ training jobì˜ ë¡œê·¸ë§Œ ë³´ëŠ” ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session.logs_for_job(job_name=job_name, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. í•™ìŠµ ê²°ê³¼ í™•ì¸\n",
    "\n",
    "í•™ìŠµì´ ì™„ë£Œëœ ë‹¤ìŒ S3ì— ì €ì¥ëœ ì‚°ì¶œë¬¼ì„ í™•ì¸í•©ë‹ˆë‹¤.<br> model ê²°ê³¼ë¬¼ì€ model.tar.gzì— ì €ì¥ë˜ì–´ ìˆê³ , ì´ì™¸ í•™ìŠµ ì¤‘ ë¡œê·¸, ê²°ê³¼ ì‚°ì¶œë¬¼ ë“±ì€ output.tar.gzì— ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "artifacts_dir = estimator.model_data.replace('model.tar.gz', '')\n",
    "print(artifacts_dir)\n",
    "!aws s3 ls --human-readable {artifacts_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> S3ì— ì €ì¥ëœ í•™ìŠµ ê²°ê³¼ ì‚°ì¶œë¬¼ì„ ëª¨ë‘ ë…¸íŠ¸ë¶ì— ë‹¤ìš´ë¡œë“œ ë°›ì€ ë‹¤ìŒ, ì••ì¶•ì„ í’‰ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dir = './neuralforecast_model'\n",
    "\n",
    "!rm -rf $model_dir\n",
    "\n",
    "import json , os\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "!aws s3 cp {artifacts_dir}model.tar.gz {model_dir}/model.tar.gz\n",
    "!tar -xvzf {model_dir}/model.tar.gz -C {model_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. í•™ìŠµ ê²°ê³¼ì˜ Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì—ëŠ” ë§ˆì§€ë§‰ ë‹¨ê³„ì— ìµœì¢… í•™ìŠµëœ ëª¨ë¸ì„ ì´ìš©í•˜ì—¬ predictë¥¼ ì‹¤í–‰í•œ ê²°ê³¼ë¥¼ real_prediction.npyì— ì €ì¥í•œ í›„ output.tar.gzë¡œ ì••ì¶•í•˜ì—¬ S3ì— ì—…ë¡œë“œ í•©ë‹ˆë‹¤. ì´ ê²°ê³¼ë¥¼ ë‹¤ì‹œ ë…¸íŠ¸ë¶ì—ì„œ loadí•œ í›„ plotí•˜ì—¬ ë³´ì—¬ì¤ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from neuralforecast.core import NeuralForecast\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nf_load = NeuralForecast.load(path=model_dir)\n",
    "Y_hat_df = nf_load.predict().reset_index()\n",
    "Y_hat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_dataset(data_dir, Y_hat_df):\n",
    "    train_df = pd.read_csv(f'{data_dir}/target_train.csv')\n",
    "    test_df = pd.read_csv(f'{data_dir}/target_test.csv')\n",
    "    related_df = pd.read_csv(f'{data_dir}/related.csv')\n",
    "    \n",
    "    data_df = pd.concat([train_df[-300:],test_df[:len(Y_hat_df)]])\n",
    "\n",
    "    data = pd.merge(data_df, related_df, on=['timestamp', 'item_id'], how='left')\n",
    "    data['timestamp'] = data['timestamp'].astype('datetime64[ns]')\n",
    "    \n",
    "    data.rename(columns = {'timestamp':'ds'},inplace=True)\n",
    "    data.rename(columns = {'item_id':'unique_id'},inplace=True)\n",
    "    data.rename(columns = {'demand':'y'},inplace=True)\n",
    "\n",
    "    return data, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "local_data_dir='./data/amzforecast/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data, test_df = prepare_dataset(local_data_dir, Y_hat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot_df = pd.concat([data, Y_hat_df]).set_index('ds') # Concatenate the train and forecast dataframes\n",
    "\n",
    "data_0 = data[data.unique_id=='casual'].reset_index(drop=True)\n",
    "data_1 = data[data.unique_id=='registered'].reset_index(drop=True)\n",
    "plot_df_0 = Y_hat_df[Y_hat_df.unique_id=='casual'].reset_index(drop=True)\n",
    "plot_df_1 = Y_hat_df[Y_hat_df.unique_id=='registered'].reset_index(drop=True)\n",
    "\n",
    "data_0 = data_0.set_index('ds')\n",
    "data_1 = data_1.set_index('ds')\n",
    "plot_df_0 = plot_df_0.set_index('ds')\n",
    "plot_df_1 = plot_df_1.set_index('ds')\n",
    "\n",
    "ax1 = plt.subplot(2, 1, 1)\n",
    "data_0[['y']].plot(linewidth=2, ax=ax1)\n",
    "plot_df_0[['NBEATS', 'NHITS']].plot(linewidth=2, ax=ax1)\n",
    "plt.title('Casual Forecast', fontsize=10)\n",
    "plt.ylabel('Hourly Passengers', fontsize=10)\n",
    "# plt.xlabel('Hourly [t]', fontsize=10)\n",
    "plt.axvline(x=plot_df_0.index[-hyperparameters['horizon']], color='k', linestyle='--', linewidth=2)\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.xticks(visible=False)\n",
    "\n",
    "ax2 = plt.subplot(2, 1, 2, sharex=ax1)\n",
    "data_1[['y']].plot(linewidth=2, ax=ax2)\n",
    "plot_df_1[['NBEATS', 'NHITS']].plot(linewidth=2, ax=ax2)\n",
    "plt.title('Registered Forecast', fontsize=10)\n",
    "plt.ylabel('Monthly Passengers', fontsize=10)\n",
    "plt.xlabel('Hourly [t]', fontsize=10)\n",
    "plt.axvline(x=plot_df_1.index[-hyperparameters['horizon']], color='k', linestyle='--', linewidth=2)\n",
    "plt.legend(prop={'size': 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trues = test_df[:len(Y_hat_df)]\n",
    "trues['timestamp'] = trues['timestamp'].astype('datetime64[ns]')\n",
    "\n",
    "trues.rename(columns = {'timestamp':'ds'},inplace=True)\n",
    "trues.rename(columns = {'item_id':'unique_id'},inplace=True)\n",
    "trues.rename(columns = {'demand':'y'},inplace=True)\n",
    "trues.set_index('ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_hat_df.set_index('ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df = pd.merge(Y_hat_df, trues, on=['ds', 'unique_id'])\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cal_metrics(pred, target):\n",
    "    return {\n",
    "        'MSE': ((pred - target) ** 2).mean(),\n",
    "        'MAE': np.abs(pred - target).mean()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = {}\n",
    "result['nbeats'] = cal_metrics(result_df['NBEATS'], result_df['y'])\n",
    "result['nhits'] = cal_metrics(result_df['NHITS'], result_df['y'])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.large",
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
